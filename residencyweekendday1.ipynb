{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXQ6J8R2Wvlv5qvqZTvbIZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinsuikyo/cumberlands/blob/main/residencyweekendday1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0_HFd2W4bG7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsWn_bxza_r_",
        "outputId": "6b021db6-fa29-4f2d-8b50-8ac635e54f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing necessary modules"
      ],
      "metadata": {
        "id": "8CnoPgJbbJuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('stopwords')  # For removing stopwords\n",
        "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging\n",
        "nltk.download('maxent_ne_chunker')  # For named entity recognition\n",
        "nltk.download('words')  # For NER word lists\n",
        "nltk.download('wordnet')  # For lemmatization\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFQcOZdZbOi8",
        "outputId": "acea84a9-4e9d-433a-eb2f-d56fcff570d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"/content/ResidencyTXT.txt\""
      ],
      "metadata": {
        "id": "jXiDZojcmPnh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_txt_as_string(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        return file.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "8A6lT6ypm3xE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = load_txt_as_string(example_string)\n"
      ],
      "metadata": {
        "id": "JVUvgZ9dm6OF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = file_content"
      ],
      "metadata": {
        "id": "ofI-Vyr3nBgt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_array = word_tokenize(example_string)"
      ],
      "metadata": {
        "id": "iM8_lAuEcfYB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCFlD7-Tcjms",
        "outputId": "ae5c5170-eb75-4984-da72-3025313bcd30"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two lists offset by one\n",
        "word_1 = word_token_array[:-1]\n",
        "word_2 = word_token_array[1:]"
      ],
      "metadata": {
        "id": "vZGHakT73sqS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Merge into bigrams\n",
        "bigrams = list(zip(word_1, word_2))\n"
      ],
      "metadata": {
        "id": "cVSyNbGG4EdO"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from nltk.util import bigrams\n"
      ],
      "metadata": {
        "id": "r8IeRGV14Uw7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram_list = list(bigrams(word_token_array))\n"
      ],
      "metadata": {
        "id": "IHUJ1sn299_7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count most common bigrams\n",
        "bigram_counts = Counter(bigrams)\n",
        "most_common_bigrams = bigram_counts.most_common(10)\n"
      ],
      "metadata": {
        "id": "faFYUHAv4JcM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert to DataFrame for display\n",
        "bigram_df = pd.DataFrame(most_common_bigrams, columns=['Bigram', 'Count'])\n"
      ],
      "metadata": {
        "id": "R28LrMjN4MWR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert DataFrame to string for output\n",
        "bigram_str = bigram_df.to_string()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vim3fRGL4PuT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save bigrams to a text file\n",
        "with open('tatenda_bigram_resday1.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(bigram_str)\n",
        "\n",
        "# Display results"
      ],
      "metadata": {
        "id": "6fjo0Sem4RiB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dZWIIuG8iAL",
        "outputId": "22698cc7-d65b-48c4-f7a1-a67d4c5cedae"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Bigram  Count\n",
            "0    (., ``)   3651\n",
            "1  (of, the)   3091\n",
            "2    (,, '')   2794\n",
            "3  (in, the)   2501\n",
            "4    (., '')   2407\n",
            "5   (., The)   2193\n",
            "6   (,, the)   2092\n",
            "7   (,, and)   1924\n",
            "8     (,, a)   1303\n",
            "9  (to, the)   1172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count the frequency of each bigram\n",
        "#bigram_counts = Counter(bigram_list)\n",
        "\n",
        "# Find the 10 most common bigrams\n",
        "#top_10_bigrams = bigram_counts.most_common(10)\n",
        "\n",
        "# Print the results\n",
        "#print(top_10_bigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk2i9mOD-CZj",
        "outputId": "cf137533-e826-497f-9643-ed8ea063bbdc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('.', '``'), 3651), (('of', 'the'), 3091), ((',', \"''\"), 2794), (('in', 'the'), 2501), (('.', \"''\"), 2407), (('.', 'The'), 2193), ((',', 'the'), 2092), ((',', 'and'), 1924), ((',', 'a'), 1303), (('to', 'the'), 1172)]\n"
          ]
        }
      ]
    }
  ]
}