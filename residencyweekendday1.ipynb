{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR1lDocFSr+DRQfLVVXcSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinsuikyo/cumberlands/blob/main/residencyweekendday1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0_HFd2W4bG7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsWn_bxza_r_",
        "outputId": "6b021db6-fa29-4f2d-8b50-8ac635e54f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing necessary modules"
      ],
      "metadata": {
        "id": "8CnoPgJbbJuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # For tokenization\n",
        "nltk.download('stopwords')  # For removing stopwords\n",
        "nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging\n",
        "nltk.download('maxent_ne_chunker')  # For named entity recognition\n",
        "nltk.download('words')  # For NER word lists\n",
        "nltk.download('wordnet')  # For lemmatization\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFQcOZdZbOi8",
        "outputId": "acea84a9-4e9d-433a-eb2f-d56fcff570d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"/content/ResidencyTXT.txt\""
      ],
      "metadata": {
        "id": "jXiDZojcmPnh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_txt_as_string(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        return file.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "8A6lT6ypm3xE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = load_txt_as_string(example_string)\n"
      ],
      "metadata": {
        "id": "JVUvgZ9dm6OF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = file_content"
      ],
      "metadata": {
        "id": "ofI-Vyr3nBgt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_array = [word for word in word_tokenize(file_content.lower()) if word.isalnum()]  # Convert to lowercase for consistency\n"
      ],
      "metadata": {
        "id": "iM8_lAuEcfYB"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCFlD7-Tcjms",
        "outputId": "219c9464-f462-4278-8bd2-010e1c1a9eee"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bigrams\n",
        "bigrams = list(zip(word_token_array[:-1], word_token_array[1:]))"
      ],
      "metadata": {
        "id": "vZGHakT73sqS"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from nltk.util import bigrams\n",
        "import re\n"
      ],
      "metadata": {
        "id": "r8IeRGV14Uw7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count most common bigrams\n",
        "if bigrams:\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    most_common_bigrams = bigram_counts.most_common(10)\n",
        "\n",
        "    bigram_df = pd.DataFrame(most_common_bigrams, columns=['Bigram', 'Count'])\n",
        "\n",
        "    bigram_str = bigram_df.to_string()\n",
        "\n",
        "    with open('tatenda_bigram_resday1.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(bigram_str)\n",
        "\n",
        "    print(bigram_df)\n",
        "else:\n",
        "    print(\"No bigrams found in the text.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faFYUHAv4JcM",
        "outputId": "63670034-ad9c-4699-ce3f-144841691e1e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Bigram  Count\n",
            "0    (of, the)   3152\n",
            "1    (in, the)   2786\n",
            "2    (to, the)   1205\n",
            "3    (on, the)   1159\n",
            "4   (for, the)    942\n",
            "5   (and, the)    865\n",
            "6      (in, a)    850\n",
            "7    (at, the)    778\n",
            "8     (to, be)    741\n",
            "9  (with, the)    618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-RZaJNTGLVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}