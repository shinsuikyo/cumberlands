# -*- coding: utf-8 -*-
"""WEEK4OP2TATENDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1375J7EFgkeyIbA-_q6xjJpDCt1vIVzB8
"""

!pip install tweepy
from google.colab import userdata

import tweepy
import time

"""Get API KEY, SECRET AND BEARER TOKEN from the X Developer portal free tier: https://developer.x.com/"""

# Twitter API credentials

bearer_token = userdata.get('XBEARER')

"""# Authenticate with the Twitter API using Bearer Token

"""

client = tweepy.Client(bearer_token=bearer_token)

"""# Define search parameters"""

query = "DeepSeek AI lang:en -is:retweet"  # Search for English tweets, excluding retweets
max_results = 100  # Fetch up to 100 tweets per request (API limit)

"""# Function to handle API rate limits and fetch tweets

"""

def fetch_tweets_with_retry(query, max_results, retries=3):
    """
    Fetch tweets from the X API with rate-limit handling.
    Args:
        query (str): The search query string.
        max_results (int): Maximum number of tweets per request.
        retries (int): Number of retries in case of rate limiting.
    Returns:
        list: List of fetched tweets.
    """
    attempt = 0
    while attempt < retries:
        try:
            # Fetch recent tweets
            response = client.search_recent_tweets(
                query=query,
                max_results=max_results,
                tweet_fields=["created_at", "text", "author_id", "public_metrics", "entities"]
            )
            return response.data if response.data else []
        except tweepy.errors.TooManyRequests:
            attempt += 1
            wait_time = 15 * 60  # Wait 15 minutes before retrying
            print(f"Rate limit reached. Retrying in {wait_time // 60} minutes...")
            time.sleep(wait_time)
    print("Max retries reached. Exiting.")
    return []

"""# Fetch tweets"""

print(f"Fetching tweets about '{query}'...\n")
tweets = fetch_tweets_with_retry(query, max_results)

if tweets:
    # Sort tweets by retweet count in descending order
    sorted_tweets = sorted(tweets, key=lambda x: x.public_metrics["retweet_count"], reverse=True)

    # Get the top 15 tweets
    top_tweets = sorted_tweets[:15]

    print(f"Top 15 Tweets about '{query}' by Retweets:\n")
    for i, tweet in enumerate(top_tweets, start=1):
        print(f"{i}. Tweet ID: {tweet.id}")
        print(f"   Author ID: {tweet.author_id}")
        print(f"   Created At: {tweet.created_at}")
        print(f"   Retweets: {tweet.public_metrics['retweet_count']}")
        print(f"   Text: {tweet.text}")
        print(f"   Mentions: {tweet.entities.get('mentions', []) if tweet.entities else 'None'}")
        print("-" * 80)
else:
    print("No tweets found or rate limit exceeded.")

""" Save Results to CSV"""

import pandas as pd

# Save the top 15 tweets to a CSV file if data exists
if tweets:
    tweet_data = [
        {
            "Tweet ID": tweet.id,
            "Author ID": tweet.author_id,
            "Created At": tweet.created_at,
            "Text": tweet.text,
            "Retweets": tweet.public_metrics["retweet_count"],
            "Mentions": tweet.entities.get("mentions", []) if tweet.entities else "None"
        }
        for tweet in top_tweets
    ]

    # Convert to DataFrame
    df = pd.DataFrame(tweet_data)

    # Save to CSV
    output_file = "top_15_deepseek_ai_tweets.csv"
    df.to_csv(output_file, index=False)
    print(f"Data saved to {output_file}")