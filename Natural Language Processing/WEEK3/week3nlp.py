# -*- coding: utf-8 -*-
"""week3nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SLjb2T-kFYLU3rPHbIR6T2sXTCTmbCl5
"""

!pip install nltk
!pip install matplotlib

"""importing necessary modules"""

import nltk
nltk.download('punkt')  # For tokenization
nltk.download('stopwords')  # For removing stopwords
nltk.download('averaged_perceptron_tagger')  # For part-of-speech tagging
nltk.download('maxent_ne_chunker')  # For named entity recognition
nltk.download('words')  # For NER word lists
nltk.download('wordnet')  # For lemmatization

from nltk.tokenize import sent_tokenize, word_tokenize

example_string = """
Muad'Dib learned rapidly because his first training was in how to learn.
And the first lesson of all was the basic trust that he could learn.
It's shocking to find how many people do not believe they can learn,
and how many more believe learning to be difficult."""

# Download the 'punkt_tab' data package
nltk.download('punkt_tab')

sent_tokenize(example_string)

word_tokenize(example_string)

nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

worf_quote = "Sir, I protest. I am not a merry man!"

words_in_quote = word_tokenize(worf_quote)
words_in_quote

stop_words = set(stopwords.words("english"))

filtered_list = []

for word in words_in_quote:
   if word.casefold() not in stop_words:
        filtered_list.append(word)

filtered_list = [
    word for word in words_in_quote if word.casefold() not in stop_words
]

filtered_list

from nltk.stem import PorterStemmer,SnowballStemmer
from nltk.tokenize import word_tokenize

stemmer = SnowballStemmer("english")

string_for_stemming = """
The crew of the USS Discovery discovered many discoveries.
Discovering is what explorers do."""

words = word_tokenize(string_for_stemming)

words

stemmed_words = [stemmer.stem(word) for word in words]

stemmed_words

from nltk.tokenize import word_tokenize

sagan_quote = """
If you wish to make an apple pie from scratch,
you must first invent the universe."""

words_in_sagan_quote = word_tokenize(sagan_quote)

nltk.download('averaged_perceptron_tagger_eng')
nltk.pos_tag(words_in_sagan_quote)

words_in_sagan_quote = word_tokenize(sagan_quote)

nltk.pos_tag(words_in_sagan_quote)

nltk.download('tagsets_json')

nltk.help.upenn_tagset()

jabberwocky_excerpt = """
'Twas brillig, and the slithy toves did gyre and gimble in the wabe:
all mimsy were the borogoves, and the mome raths outgrabe."""

words_in_excerpt = word_tokenize(jabberwocky_excerpt)

nltk.pos_tag(words_in_excerpt)

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

lemmatizer.lemmatize("scarves")

string_for_lemmatizing = "The friends of DeSoto love scarves."

words = word_tokenize(string_for_lemmatizing)

words

lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

lemmatized_words

lemmatizer.lemmatize("worst")

lemmatizer.lemmatize("worst", pos="a")

lotr_quote = "It's a dangerous business, Frodo, going out your door."

words_in_lotr_quote = word_tokenize(lotr_quote)
words_in_lotr_quote

nltk.download("averaged_perceptron_tagger")
lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)
lotr_pos_tags

grammar = "NP: {<DT>?<JJ>*<NN>}"

chunk_parser = nltk.RegexpParser(grammar)

tree = chunk_parser.parse(lotr_pos_tags)

!pip install svgling
from IPython.display import display
from svgling import draw_tree
import nltk

# Assuming 'tree' is your NLTK tree object
display(draw_tree(tree))

lotr_pos_tags

grammar = """
Chunk: {<.*>+}
       }<JJ>{"""

chunk_parser = nltk.RegexpParser(grammar)

tree = chunk_parser.parse(lotr_pos_tags)

!pip install svgling

tree

!pip install svgling
from IPython.display import display
from svgling import draw_tree

display(draw_tree(tree))  # Replace 'tree' with your actual tree object

nltk.download("maxent_ne_chunker")
nltk.download("words")
nltk.download('maxent_ne_chunker_tab')
tree = nltk.ne_chunk(lotr_pos_tags)

tree

!pip install svgling
from IPython.display import display
from svgling import draw_tree
import nltk

tree = nltk.ne_chunk(lotr_pos_tags, binary=True)
# Instead of tree.draw(), use the following:
display(draw_tree(tree))

quote = """
Men like Schiaparelli watched the red planet—it is odd, by-the-bye, that
for countless centuries Mars has been the star of war—but failed to
interpret the fluctuating appearances of the markings they mapped so well.
All that time the Martians must have been getting ready.

During the opposition of 1894 a great light was seen on the illuminated
part of the disk, first at the Lick Observatory, then by Perrotin of Nice,
and then by other observers. English readers heard of it first in the
issue of Nature dated August 2."""

def extract_ne(quote, language='english'):
    words = word_tokenize(quote, language=language)
    tags = nltk.pos_tag(words)
    tree = nltk.ne_chunk(tags, binary=True)
    return set(
        " ".join(i[0] for i in t)
        for t in tree
        if hasattr(t, "label") and t.label() == "NE"
    )

extract_ne(quote)

nltk.download("book")
from nltk.book import *

text8.concordance("man")

text8.concordance("woman")

text8.dispersion_plot(
    ["woman", "lady", "girl", "gal", "man", "gentleman", "boy", "guy"]
)

text2.dispersion_plot(["Allenham", "Whitwell", "Cleveland", "Combe"])

from nltk import FreqDist

frequency_distribution = FreqDist(text8)
print(frequency_distribution)

frequency_distribution.most_common(20)

meaningful_words = [
    word for word in text8 if word.casefold() not in stop_words
]

frequency_distribution = FreqDist(meaningful_words)

frequency_distribution.most_common(20)

frequency_distribution.plot(20, cumulative=True)

text8.collocations()

lemmatized_words = [lemmatizer.lemmatize(word) for word in text8]

new_text = nltk.Text(lemmatized_words)

new_text.collocations()